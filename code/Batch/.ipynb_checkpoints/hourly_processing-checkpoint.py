{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fa1955c-f4b6-41cb-8027-dadea25bf2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "try:\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data Processing and Insertion\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define base HDFS directory\n",
    "base_hdfs_dir = \"/user/itversity/raw_layer\"\n",
    "\n",
    "# Calculate current date and previous hour\n",
    "current_dt = datetime.now()\n",
    "\n",
    "# Handle special case when current time is 12am (midnight)\n",
    "if current_dt.hour == 0:\n",
    "    # Adjust processing date to previous day\n",
    "    processing_date = (current_dt - timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "    prev_hour_str = \"23\"  # Previous hour is 11pm\n",
    "else:\n",
    "    processing_date = current_dt.strftime(\"%Y%m%d\")\n",
    "    prev_hour_str = (current_dt - timedelta(hours=1)).strftime(\"%H\")\n",
    "\n",
    "# Construct input path in HDFS\n",
    "input_dir_pattern = f\"{base_hdfs_dir}/{processing_date}/{prev_hour_str}_*\"\n",
    "\n",
    "# Get all directories matching the input pattern\n",
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "input_dirs = [status.getPath().toString() for status in fs.globStatus(spark._jvm.org.apache.hadoop.fs.Path(input_dir_pattern))]\n",
    "\n",
    "# Lists to store DataFrames for each category\n",
    "branches_dfs = []\n",
    "agents_dfs = []\n",
    "transactions_dfs = []\n",
    "\n",
    "# Load existing data from Hive tables\n",
    "branches_hive_df = spark.table(\"BigData_DWH.branches_dimension\")\n",
    "agents_hive_df = spark.table(\"BigData_DWH.sales_agents_dimension\")\n",
    "transactions_hive_df = spark.table(\"BigData_DWH.Transactions_Fact_table\")\n",
    "\n",
    "for input_dir in input_dirs:\n",
    "    # Read all files in the directory\n",
    "    input_files = [status.getPath().toString() for status in fs.listStatus(spark._jvm.org.apache.hadoop.fs.Path(input_dir))]\n",
    "\n",
    "    for input_file in input_files:\n",
    "        # Extract the file name from the input path\n",
    "        input_file_name = os.path.basename(input_file)\n",
    "\n",
    "        # Read the DataFrame from CSV\n",
    "        df = spark.read.format(\"csv\").option(\"header\", \"true\").load(input_file)\n",
    "\n",
    "\n",
    "        # Determine DataFrame name based on file name content\n",
    "        if \"branches\" in input_file_name.lower():\n",
    "            branches_dfs.append(df)\n",
    "        elif \"agents\" in input_file_name.lower():\n",
    "            agents_dfs.append(df)\n",
    "        elif \"transactions\" in input_file_name.lower():\n",
    "            transactions_dfs.append(df)\n",
    "        else:\n",
    "            continue  # Skip processing for unknown files\n",
    "\n",
    "if branches_dfs:\n",
    "    branches_new_df = branches_dfs[0]\n",
    "    for df in branches_dfs[1:]:\n",
    "        branches_new_df = branches_new_df.union(df)\n",
    "\n",
    "    branches_new_df = branches_new_df.dropDuplicates()\n",
    "    branches_new_df = branches_new_df.join(branches_hive_df, on=\"branch_id\", how=\"left_anti\")\n",
    "\n",
    "\n",
    "if agents_dfs:\n",
    "    agents_new_df = agents_dfs[0]\n",
    "    for df in agents_dfs[1:]:\n",
    "        agents_new_df = agents_new_df.union(df)\n",
    "\n",
    "    agents_new_df = agents_new_df.dropDuplicates()\n",
    "    agents_new_df = agents_new_df.join(agents_hive_df, on=\"sales_person_id\", how=\"left_anti\")\n",
    "    \n",
    "\n",
    "# Print column names of transactions_dfs\n",
    "if transactions_dfs:\n",
    "    transactions_new_df = transactions_dfs[0]\n",
    "    for df in transactions_dfs[1:]:\n",
    "        transactions_new_df = transactions_new_df.union(df)\n",
    "\n",
    "    transactions_new_df = transactions_new_df.dropDuplicates()\n",
    "    transactions_new_df = transactions_new_df.join(transactions_hive_df, on=\"transaction_id\", how=\"left_anti\")\n",
    "\n",
    "\n",
    "# Columns to be moved to a new DataFrame\n",
    "columns_to_move = [\"customer_id\", \"customer_fname\", \"cusomter_lname\", \"cusomter_email\"]\n",
    "\n",
    "# Create a new DataFrame with the selected columns\n",
    "customer_df = transactions_new_df.select(columns_to_move)\n",
    "\n",
    "\n",
    "# Columns to be moved to a new DataFrame\n",
    "columns_to_move = [\"product_id\", \"product_name\", \"product_category\"]\n",
    "\n",
    "# Create a new DataFrame with the selected columns\n",
    "product_df = transactions_new_df.select(columns_to_move)\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import when, col, coalesce\n",
    "\n",
    "transactions_new_df = transactions_new_df.withColumn(\n",
    "    \"offer\",\n",
    "    coalesce(\n",
    "        when(col(\"offer_1\") == \"True\", \"offer_1\"),\n",
    "        when(col(\"offer_2\") == \"True\", \"offer_2\"),\n",
    "        when(col(\"offer_3\") == \"True\", \"offer_3\"),\n",
    "        when(col(\"offer_4\") == \"True\", \"offer_4\"),\n",
    "        when(col(\"offer_5\") == \"True\", \"offer_5\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Drop the original offer columns and other specified columns\n",
    "transactions_new_df = transactions_new_df.drop(\"offer_1\", \"offer_2\", \"offer_3\", \"offer_4\", \"offer_5\")\n",
    "\n",
    "\n",
    "transactions_new_df = transactions_new_df.drop(\"logs\", \"source\")\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import when, col, coalesce, expr\n",
    "\n",
    "# Calculate Total_price as DOUBLE\n",
    "transactions_new_df = transactions_new_df.withColumn(\n",
    "    \"Total_price\",\n",
    "    col(\"units\") * col(\"unit_price\")\n",
    ")\n",
    "\n",
    "# Add 'discount_percentage' column\n",
    "transactions_new_df = transactions_new_df.withColumn(\n",
    "    \"discount_percentage\",\n",
    "    when(col(\"offer\") == \"offer_1\", 0.05)\n",
    "    .when(col(\"offer\") == \"offer_2\", 0.10)\n",
    "    .when(col(\"offer\") == \"offer_3\", 0.15)\n",
    "    .when(col(\"offer\") == \"offer_4\", 0.20)\n",
    "    .when(col(\"offer\") == \"offer_5\", 0.25)\n",
    "    .otherwise(0.0)\n",
    ")\n",
    "\n",
    "# Add 'Total_price_after_discount' column\n",
    "transactions_new_df = transactions_new_df.withColumn(\n",
    "    \"Total_price_after_discount\",\n",
    "    col(\"Total_price\") * (1 - col(\"discount_percentage\"))\n",
    ")\n",
    "\n",
    "# Columns to be moved to a new DataFrame\n",
    "columns_to_move = [\"transaction_id\", \"unit_price\", \"is_online\", \"payment_method\", \"shipping_address\", \"offer\"]\n",
    "\n",
    "# Create a new DataFrame with the selected columns\n",
    "orders_df = transactions_new_df.select(columns_to_move)\n",
    "\n",
    "# Select only the necessary columns\n",
    "transactions_new_df = transactions_new_df.select(\n",
    "    \"transaction_date\",\n",
    "    \"transaction_id\",\n",
    "    \"customer_id\",\n",
    "    \"sales_agent_id\",\n",
    "    \"branch_id\",\n",
    "    \"product_id\",\n",
    "    \"units\",\n",
    "    \"Total_price\",\n",
    "    \"discount_percentage\",\n",
    "    \"Total_price_after_discount\"\n",
    ")\n",
    "\n",
    "# Perform casting\n",
    "transactions_new_df = transactions_new_df.withColumn(\n",
    "    \"units\",\n",
    "    col(\"units\").cast(\"int\")\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "orders_df.createOrReplaceTempView(\"orders_Dim\")\n",
    "transactions_new_df.createOrReplaceTempView(\"transactions_Fact\")\n",
    "product_df.createOrReplaceTempView(\"product_Dim\")\n",
    "branches_new_df.createOrReplaceTempView(\"branches_view\")\n",
    "agents_new_df.createOrReplaceTempView(\"agents_view\")\n",
    "customer_df.createOrReplaceTempView(\"customer_dim\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO BigData_DWH.Transactions_Fact_table\n",
    "    PARTITION (transaction_date)\n",
    "    SELECT\n",
    "        transaction_id,\n",
    "        customer_id,\n",
    "        sales_agent_id,\n",
    "        branch_id,\n",
    "        product_id,\n",
    "        units,\n",
    "        Total_price,\n",
    "        discount_percentage,\n",
    "        Total_price_after_discount,\n",
    "        transaction_date\n",
    "    FROM transactions_Fact\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"INSERT INTO BigData_DWH.customers_dimension SELECT * FROM customer_dim\")\n",
    "\n",
    "spark.sql(\"INSERT INTO BigData_DWH.orders_dimension SELECT * FROM orders_Dim\")\n",
    "\n",
    "spark.sql(\"INSERT INTO BigData_DWH.products_dimension SELECT * FROM product_Dim\")\n",
    "\n",
    "spark.sql(\"INSERT INTO BigData_DWH.branches_dimension SELECT * FROM branches_view\")\n",
    "\n",
    "spark.sql(\"INSERT INTO BigData_DWH.sales_agents_dimension SELECT * FROM agents_view\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\", file=sys.stderr)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "id": "4fad374c-884a-4de8-9dae-b5d3d3c14127",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
