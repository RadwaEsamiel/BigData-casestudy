{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c77b00cc-80fc-4a21-adee-fa71b43f13cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.functions import from_json, when, col, to_date, date_format\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, TimestampType, MapType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a2fb2d3-2835-4b9a-9289-3973a3acc984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "outputDir = \"hdfs://localhost:9000/user/itversity/stream_output\"\n",
    "checkpointDir = \"hdfs://localhost:9000/user/itversity/stream_checkpoint/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33241a0c-51f4-4e8e-84b0-20c408205b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaStreamingExample\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema for the incoming JSON data\n",
    "schema = StructType() \\\n",
    "    .add(\"eventType\", StringType()) \\\n",
    "    .add(\"customerId\", StringType()) \\\n",
    "    .add(\"productId\", StringType()) \\\n",
    "    .add(\"timestamp\", StringType()) \\\n",
    "    .add(\"metadata\", MapType(StringType(), StringType())) \\\n",
    "    .add(\"quantity\", IntegerType()) \\\n",
    "    .add(\"totalAmount\", FloatType()) \\\n",
    "    .add(\"paymentMethod\", StringType()) \\\n",
    "    .add(\"recommendedProductId\", StringType()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ef24e83-68c3-49e3-a2c5-79504b1b98c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to create and verify an external table\n",
    "def create_and_verify_table(create_table_query, table_name, schema_name, path):\n",
    "    try:\n",
    "        # Drop table if it exists\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {schema_name}.{table_name}\")\n",
    "        \n",
    "        # Create new external table with schema and location\n",
    "        full_create_query = f\"\"\"\n",
    "        CREATE EXTERNAL TABLE {schema_name}.{table_name} {create_table_query}\n",
    "        STORED AS PARQUET\n",
    "        LOCATION '{path}'\n",
    "        \"\"\"\n",
    "        spark.sql(full_create_query)\n",
    "        \n",
    "        # Verify table creation\n",
    "        table_exists = spark.sql(f\"SHOW TABLES IN {schema_name} LIKE '{table_name}'\").count() > 0\n",
    "        if table_exists:\n",
    "            print(f\"Success: Table '{schema_name}.{table_name}' created successfully at {path}.\")\n",
    "        else:\n",
    "            print(f\"Failure: Table '{schema_name}.{table_name}' creation failed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating table '{schema_name}.{table_name}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aff1d32-97c7-47df-9470-4f47e79c29e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Table 'BigData_DWH.stream_output' created successfully at hdfs://localhost:9000/user/itversity/stream_output.\n"
     ]
    }
   ],
   "source": [
    "create_table_query = \"\"\"\n",
    "(\n",
    "    eventType STRING,\n",
    "    customerId STRING,\n",
    "    productId STRING,\n",
    "    timestamp STRING,\n",
    "    quantity INT,\n",
    "    totalAmount FLOAT,\n",
    "    paymentMethod STRING,\n",
    "    recommendedProductId STRING,\n",
    "    category STRING,\n",
    "    source STRING\n",
    ")\n",
    "PARTITIONED BY (date DATE)\n",
    "\"\"\"\n",
    "\n",
    "# Call the function to create and verify the table\n",
    "create_and_verify_table(create_table_query, \"stream_output\", \"BigData_DWH\", outputDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72e28c51-4c55-4b7a-9cc3-084f36c8b34a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f62f6ef39d34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark3/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark3/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark3/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark3/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Kafka connection details\n",
    "bootstrap_servers = \"pkc-56d1g.eastus.azure.confluent.cloud:9092\"\n",
    "kafka_topic = \"mark_topic\"  # add your Kafka topic name\n",
    "kafka_username = \"JUKQQM4ZM632RECA\"\n",
    "kafka_password = \"UUkrPuSttgOC0U9lY3ZansNsKfN9fbxZPFwrGxudDrfv+knTD4rCwK+KdIzVPX0D\"\n",
    "\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaToParquet\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data from Kafka topic as a streaming DataFrame\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\") \\\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n",
    "    .option(\"kafka.sasl.jaas.config\",\n",
    "            f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{kafka_username}\" password=\"{kafka_password}\";') \\\n",
    "    .load()\n",
    "\n",
    "# Parse the JSON data\n",
    "json_df = df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "            .select(from_json(\"value\", schema).alias(\"data\")) \\\n",
    "            .select(\"data.*\")\n",
    "\n",
    "# Split the metadata column into category and source\n",
    "json_df2 = json_df.withColumn(\"category\", col(\"metadata\")[\"category\"]) \\\n",
    "                    .withColumn(\"source\", col(\"metadata\")[\"source\"]) \\\n",
    "                    .drop(\"metadata\")\n",
    "\n",
    "# Add a new column with the date extracted from timestamp\n",
    "partitioned_df = json_df2.withColumn(\"date\", to_date(col(\"timestamp\")))\n",
    "\n",
    "\n",
    "\n",
    "# Write the stream to HDFS partitioned by 'date'\n",
    "query = partitioned_df \\\n",
    "    .writeStream \\\n",
    "    .partitionBy(\"date\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", outputDir) \\\n",
    "    .option(\"checkpointLocation\", checkpointDir) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1727b1fe-f755-4a8c-a270-2c0adfa7e044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+---------+-------------------+--------+-----------+-------------+--------------------+--------+------+----------+\n",
      "|          eventType|customerId|productId|          timestamp|quantity|totalAmount|paymentMethod|recommendedProductId|category|source|      date|\n",
      "+-------------------+----------+---------+-------------------+--------+-----------+-------------+--------------------+--------+------+----------+\n",
      "|        productView|     19081|     4048|2024-07-05T07:31:42|    null|       null|         null|                null|Clothing|Direct|2024-07-05|\n",
      "|recommendationClick|     75455|     4928|2024-07-05T07:31:44|    null|       null|         null|                2818|    null|  null|2024-07-05|\n",
      "|          addToCart|     68726|     3266|2024-07-05T07:31:46|       3|       null|         null|                null|    null|  null|2024-07-05|\n",
      "|           purchase|     56748|     1431|2024-07-05T07:31:48|       5|     384.24|       PayPal|                null|    null|  null|2024-07-05|\n",
      "|        productView|     40110|     6500|2024-07-05T07:31:54|    null|       null|         null|                null|Clothing|Direct|2024-07-05|\n",
      "+-------------------+----------+---------+-------------------+--------+-----------+-------------+--------------------+--------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(outputDir)\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0937b06f-74a1-4834-a26a-2aff2d382a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"MSCK REPAIR TABLE BigData_DWH.stream_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db4124e2-1ca7-4a3c-b5ce-2d1ee65865ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count in the table:\n",
      "+---------+\n",
      "|row_count|\n",
      "+---------+\n",
      "|      449|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema_name=\"BigData_DWH\"\n",
    "table_name=\"stream_output\"\n",
    "\n",
    "# 1. Check if the Hive table has data\n",
    "row_count_query = f\"SELECT COUNT(*) AS row_count FROM {schema_name}.{table_name}\"\n",
    "row_count_result = spark.sql(row_count_query)\n",
    "print(\"Row count in the table:\")\n",
    "row_count_result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9465b1ce-3384-41e1-8b4c-3fb582e2dec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data from the table:\n",
      "+-----------+----------+---------+-------------------+--------+-----------+-------------+--------------------+--------+-------------+----------+\n",
      "|  eventType|customerId|productId|          timestamp|quantity|totalAmount|paymentMethod|recommendedProductId|category|       source|      date|\n",
      "+-----------+----------+---------+-------------------+--------+-----------+-------------+--------------------+--------+-------------+----------+\n",
      "|  addToCart|     60945|     3783|2024-07-04T17:51:40|       2|       null|         null|                null|    null|         null|2024-07-04|\n",
      "|productView|     90819|     7855|2024-07-04T17:51:41|    null|       null|         null|                null|Clothing|Advertisement|2024-07-04|\n",
      "|  addToCart|     66955|     4881|2024-07-04T17:51:43|       2|       null|         null|                null|    null|         null|2024-07-04|\n",
      "|   purchase|     48504|     3564|2024-07-04T17:51:47|       1|     424.63|   Debit Card|                null|    null|         null|2024-07-04|\n",
      "|   purchase|     29209|     1030|2024-07-04T17:51:50|       3|     371.67|       PayPal|                null|    null|         null|2024-07-04|\n",
      "+-----------+----------+---------+-------------------+--------+-----------+-------------+--------------------+--------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_data_query = f\"SELECT * FROM {schema_name}.{table_name} LIMIT 5\"\n",
    "sample_data_result = spark.sql(sample_data_query)\n",
    "print(\"Sample data from the table:\")\n",
    "sample_data_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10179a9c-124f-4499-88b2-5f1fcff97cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+-------------------+\n",
      "|productId|total_sales_amount|total_quantity_sold|\n",
      "+---------+------------------+-------------------+\n",
      "|     1322| 830.2200012207031|                  7|\n",
      "|     1309| 635.3699951171875|                  4|\n",
      "|     8059| 499.8999938964844|                  5|\n",
      "|     8084| 495.6099853515625|                  5|\n",
      "|     3950| 488.6400146484375|                  4|\n",
      "|     9783| 481.5799865722656|                  2|\n",
      "|     8367| 477.7300109863281|                  2|\n",
      "|     5907| 474.2200012207031|                  2|\n",
      "|     1241| 468.9599914550781|                  5|\n",
      "|     1446| 468.5399932861328|                  4|\n",
      "|     1064|468.04998779296875|                  3|\n",
      "|     1424|466.05999755859375|                  1|\n",
      "|     6219| 460.7099914550781|                  3|\n",
      "|     1153| 455.1199951171875|                  5|\n",
      "|     1264| 453.1099853515625|                  4|\n",
      "|     1013| 451.2799987792969|                  3|\n",
      "|     1282| 448.9700012207031|                  2|\n",
      "|     1126| 432.8399963378906|                  5|\n",
      "|     3564| 424.6300048828125|                  1|\n",
      "|     1567|  423.510009765625|                  2|\n",
      "+---------+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query1 = \"\"\"\n",
    "SELECT \n",
    "    productId,\n",
    "    SUM(totalAmount) AS total_sales_amount,\n",
    "    SUM(quantity) AS total_quantity_sold\n",
    "FROM \n",
    "    BigData_DWH.stream_output\n",
    "GROUP BY \n",
    "    productId\n",
    "ORDER BY \n",
    "    total_sales_amount DESC\n",
    "\"\"\"\n",
    "\n",
    "# Execute Query 1\n",
    "result1 = spark.sql(query1)\n",
    "result1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da48b0a8-8f82-48af-a23e-50f99603188c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+------------------+-------------------+\n",
      "|      date|paymentMethod|daily_sales_amount|daily_quantity_sold|\n",
      "+----------+-------------+------------------+-------------------+\n",
      "|2024-07-04|  Credit Card|3293.6600189208984|                 37|\n",
      "|2024-07-04|   Debit Card|3099.7600326538086|                 26|\n",
      "|2024-07-04|       PayPal| 2838.580047607422|                 42|\n",
      "|2024-07-04|         null|              null|                130|\n",
      "|2024-07-05|  Credit Card| 6837.699935913086|                 69|\n",
      "|2024-07-05|       PayPal| 6566.679931640625|                 85|\n",
      "|2024-07-05|   Debit Card| 4298.559982299805|                 64|\n",
      "|2024-07-05|         null|              null|                225|\n",
      "+----------+-------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 2: Daily Sales and Quantity by Payment Method\n",
    "query2 = \"\"\"\n",
    "SELECT \n",
    "    date,\n",
    "    paymentMethod,\n",
    "    SUM(totalAmount) AS daily_sales_amount,\n",
    "    SUM(quantity) AS daily_quantity_sold\n",
    "FROM \n",
    "    BigData_DWH.stream_output\n",
    "GROUP BY \n",
    "    date, paymentMethod\n",
    "ORDER BY \n",
    "    date, daily_sales_amount DESC\n",
    "\"\"\"\n",
    "\n",
    "# Execute Query 2\n",
    "result2 = spark.sql(query2)\n",
    "result2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93b7fc4-5be9-43f7-8c6d-7f92aee3f527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5f43b11-1c3a-45be-a973-032379b121a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe36bdf1-121c-4c1a-98d2-fa77b1442bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
