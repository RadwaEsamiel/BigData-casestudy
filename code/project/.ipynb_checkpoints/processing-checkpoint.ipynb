{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9f76d78-0753-4c09-8217-a3dbcac54a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c690cc6-958a-4f48-b7a9-6cdcba3243ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data Processing and Insertion\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1524bd7-b61f-44ba-8de0-9eec3fdec305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base HDFS directory\n",
    "base_hdfs_dir = \"/user/itversity/raw_layer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8004add-0cff-4204-a723-c050c732c11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate current date and previous hour\n",
    "current_dt = datetime.now()\n",
    "\n",
    "# Handle special case when current time is 12am (midnight)\n",
    "if current_dt.hour == 0:\n",
    "    # Adjust processing date to previous day\n",
    "    processing_date = (current_dt - timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "    prev_hour_str = \"23\"  # Previous hour is 11pm\n",
    "else:\n",
    "    processing_date = current_dt.strftime(\"%Y%m%d\")\n",
    "    prev_hour_str = (current_dt - timedelta(hours=1)).strftime(\"%H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7838f786-f62f-439f-9f20-fe63d3f5a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct input path in HDFS\n",
    "input_dir_pattern = f\"{base_hdfs_dir}/{processing_date}/{prev_hour_str}_*\"\n",
    "\n",
    "# Get all directories matching the input pattern\n",
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "input_dirs = [status.getPath().toString() for status in fs.globStatus(spark._jvm.org.apache.hadoop.fs.Path(input_dir_pattern))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "201f9300-3332-4488-8df0-f9f516ec9390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store DataFrames for each category\n",
    "branches_dfs = []\n",
    "agents_dfs = []\n",
    "transactions_dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb840d80-a7f1-41a5-8e9c-2cc2b2077a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing data from Hive tables\n",
    "branches_hive_df = spark.table(\"BigData_DWH.branches_dimension\")\n",
    "agents_hive_df = spark.table(\"BigData_DWH.sales_agents_dimension\")\n",
    "transactions_hive_df = spark.table(\"BigData_DWH.Transactions_Fact_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edde9fe4-f5d8-463f-ac75-d09885dedf2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for input_dir in input_dirs:\n",
    "    # Read all files in the directory\n",
    "    input_files = [status.getPath().toString() for status in fs.listStatus(spark._jvm.org.apache.hadoop.fs.Path(input_dir))]\n",
    "\n",
    "    for input_file in input_files:\n",
    "        # Extract the file name from the input path\n",
    "        input_file_name = os.path.basename(input_file)\n",
    "\n",
    "        # Read the DataFrame from CSV\n",
    "        df = spark.read.format(\"csv\").option(\"header\", \"true\").load(input_file)\n",
    "\n",
    "        # Show the DataFrame (for verification)\n",
    "        print(f\"DataFrame from {input_file_name}:\")\n",
    "        df.show()\n",
    "\n",
    "        # Determine DataFrame name based on file name content\n",
    "        if \"branches\" in input_file_name.lower():\n",
    "            branches_dfs.append(df)\n",
    "        elif \"agents\" in input_file_name.lower():\n",
    "            agents_dfs.append(df)\n",
    "        elif \"transactions\" in input_file_name.lower():\n",
    "            transactions_dfs.append(df)\n",
    "        else:\n",
    "            continue  # Skip processing for unknown files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04ab61bf-abf6-4b28-b06d-bdb3ec3eca30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if branches_dfs:\n",
    "    branches_new_df = branches_dfs[0]\n",
    "    for df in branches_dfs[1:]:\n",
    "        branches_new_df = branches_new_df.union(df)\n",
    "\n",
    "    branches_new_df = branches_new_df.dropDuplicates()\n",
    "    branches_new_df = branches_new_df.join(branches_hive_df, on=\"branch_id\", how=\"left_anti\")\n",
    "    print(\"branches DataFrame:\")\n",
    "    branches_new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d128253-8231-4fb6-897c-cdcbab7ed1e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if agents_dfs:\n",
    "    agents_new_df = agents_dfs[0]\n",
    "    for df in agents_dfs[1:]:\n",
    "        agents_new_df = agents_new_df.union(df)\n",
    "\n",
    "    agents_new_df = agents_new_df.dropDuplicates()\n",
    "    agents_new_df = agents_new_df.join(agents_hive_df, on=\"sales_person_id\", how=\"left_anti\")\n",
    "    \n",
    "    print(\"agents DataFrame:\")\n",
    "    agents_new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9299e97d-b21c-4cc2-aa3a-28d3357c3ba7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print column names of transactions_dfs\n",
    "if transactions_dfs:\n",
    "    transactions_new_df = transactions_dfs[0]\n",
    "    for df in transactions_dfs[1:]:\n",
    "        transactions_new_df = transactions_new_df.union(df)\n",
    "\n",
    "    transactions_new_df = transactions_new_df.dropDuplicates()\n",
    "    transactions_new_df = transactions_new_df.join(transactions_hive_df, on=\"transaction_id\", how=\"left_anti\")\n",
    "    \n",
    "    print(\"Transactions DataFrame:\")\n",
    "    transactions_new_df.show()\n",
    "    \n",
    "    # Get column names\n",
    "    column_names = transactions_new_df.columns\n",
    "    print(\"Column names:\")\n",
    "    for col in column_names:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87e3e9f3-281c-4661-9257-f3980a57571f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transactions_new_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e8263d1c303f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create a new DataFrame with the selected columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcustomer_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransactions_new_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns_to_move\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Customer DataFrame:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcustomer_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transactions_new_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Columns to be moved to a new DataFrame\n",
    "columns_to_move = [\"customer_id\", \"customer_fname\", \"cusomter_lname\", \"cusomter_email\"]\n",
    "\n",
    "# Create a new DataFrame with the selected columns\n",
    "customer_df = transactions_new_df.select(columns_to_move)\n",
    "print(\"Customer DataFrame:\")\n",
    "customer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876f41b1-b144-43c5-b3a8-765e56ba97cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.createOrReplaceTempView(\"customer_dim\")\n",
    "# SQL query to get the most selling products\n",
    "\n",
    "customer_dim = \"\"\"\n",
    "SELECT * FROM customer_dim\n",
    "\"\"\"\n",
    "\n",
    "customer_dim = spark.sql(customer_dim)\n",
    "\n",
    "customer_dim.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c5bcbf-b26a-47e1-b390-dcc798c2f713",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"INSERT INTO BigData_DWH.customers_dimension SELECT * FROM customer_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a43b4d4-07a5-4b44-8061-789f369d3a96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Columns to be moved to a new DataFrame\n",
    "columns_to_move = [\"product_id\", \"product_name\", \"product_category\"]\n",
    "\n",
    "# Create a new DataFrame with the selected columns\n",
    "product_df = transactions_new_df.select(columns_to_move)\n",
    "print(\"Customer DataFrame:\")\n",
    "product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6fb13a-db14-46fe-bad6-1b2b659f8e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df.createOrReplaceTempView(\"product_Dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e3e37f-c417-48f1-aa4f-5f7aa076809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"INSERT INTO BigData_DWH.products_dimension SELECT * FROM product_Dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e000e84-7592-4cc8-a1e3-ea7ec1503aec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, coalesce\n",
    "\n",
    "transactions_new_df = transactions_new_df.withColumn(\n",
    "    \"offer\",\n",
    "    coalesce(\n",
    "        when(col(\"offer_1\") == \"True\", \"offer_1\"),\n",
    "        when(col(\"offer_2\") == \"True\", \"offer_2\"),\n",
    "        when(col(\"offer_3\") == \"True\", \"offer_3\"),\n",
    "        when(col(\"offer_4\") == \"True\", \"offer_4\"),\n",
    "        when(col(\"offer_5\") == \"True\", \"offer_5\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Drop the original offer columns and other specified columns\n",
    "transactions_new_df = transactions_new_df.drop(\"offer_1\", \"offer_2\", \"offer_3\", \"offer_4\", \"offer_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfd2ea1-4da8-448e-b843-db411b4fb00f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show the transformed DataFrame\n",
    "transactions_new_df.show()\n",
    "transactions_new_df = transactions_new_df.drop(\"logs\", \"source\")\n",
    "# Get column names\n",
    "column_names = transactions_new_df.columns\n",
    "print(\"Column names:\")\n",
    "for col in column_names:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e55bf2c-8352-4251-8935-987cc4366c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, coalesce,expr\n",
    "\n",
    "# Calculate Total_price as DOUBLE\n",
    "transactions_new_df = transactions_new_df.withColumn(\n",
    "    \"Total_price\",\n",
    "     col(\"units\") * col(\"unit_price\")\n",
    ")\n",
    "\n",
    "# Show the updated DataFrame\n",
    "transactions_new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b541d6da-9078-4568-bde0-9ce2b53ddbf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    # Add 'discount_percentage' column\n",
    "    transactions_new_df = transactions_new_df.withColumn(\n",
    "        \"discount_percentage\",\n",
    "        when(col(\"offer\") == \"offer_1\", 0.05)\n",
    "        .when(col(\"offer\") == \"offer_2\", 0.10)\n",
    "        .when(col(\"offer\") == \"offer_3\", 0.15)\n",
    "        .when(col(\"offer\") == \"offer_4\", 0.20)\n",
    "        .when(col(\"offer\") == \"offer_5\", 0.25)\n",
    "        .otherwise(0.0)\n",
    "    )\n",
    "\n",
    "    # Add 'Total_price_paid_after_discount' column\n",
    "    transactions_new_df = transactions_new_df.withColumn(\n",
    "        \"Total_price_after_discount\",\n",
    "        col(\"Total_price\") * (1 - col(\"discount_percentage\"))\n",
    "    )\n",
    "\n",
    "    # Show the transformed DataFrame\n",
    "    transactions_new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcf0f97-cea6-4c10-99e3-79918b819011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_names = transactions_new_df.columns\n",
    "print(\"Column names:\")\n",
    "for col in column_names:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158f3ccc-6482-413d-973f-2605a8020b8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Columns to be moved to a new DataFrame\n",
    "columns_to_move = [\"transaction_id\", \"unit_price\", \"is_online\", \"payment_method\", \"shipping_address\", \"offer\"]\n",
    "\n",
    "# Create a new DataFrame with the selected columns\n",
    "orders_df = transactions_new_df.select(columns_to_move)\n",
    "print(\"orders DataFrame:\")\n",
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabefe1f-7ba3-4e35-a240-1f26eff4d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.createOrReplaceTempView(\"orders_Dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fbef0e-796e-4986-b862-7b13bc40d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"INSERT INTO BigData_DWH.orders_dimension SELECT * FROM orders_Dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23f792a-9d6a-4e67-aa1f-29413f8b6019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, coalesce, expr\n",
    "\n",
    "# Select only the necessary columns\n",
    "transactions_new_dsf = transactions_new_df.select(\n",
    "    \"transaction_date\",\n",
    "    \"transaction_id\",\n",
    "    \"customer_id\",\n",
    "    \"sales_agent_id\",\n",
    "    \"branch_id\",\n",
    "    \"product_id\",\n",
    "    \"units\",\n",
    "    \"Total_price\",\n",
    "    \"discount_percentage\",\n",
    "    \"Total_price_after_discount\"\n",
    ")\n",
    "transactions_new_dsf.show()\n",
    "\n",
    "# Perform casting\n",
    "transactions_new_dsf = transactions_new_dsf.withColumn(\n",
    "    \"units\",\n",
    "    col(\"units\").cast(\"int\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bdba71-09cf-4ab6-a23f-5d8bdb7cc3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_new_dsf.createOrReplaceTempView(\"transactions_Fact\")\n",
    "branches_new_df.createOrReplaceTempView(\"branches_view\")\n",
    "agents_new_df.createOrReplaceTempView(\"agents_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e35e4-4293-4ee1-9db2-9f4f0064b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    INSERT INTO BigData_DWH.Transactions_Fact_table\n",
    "    PARTITION (transaction_date)\n",
    "    SELECT\n",
    "        transaction_id,\n",
    "        customer_id,\n",
    "        sales_agent_id,\n",
    "        branch_id,\n",
    "        product_id,\n",
    "        units,\n",
    "        Total_price,\n",
    "        discount_percentage,\n",
    "        Total_price_after_discount,\n",
    "        transaction_date\n",
    "    FROM transactions_Fact\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684f31a0-2602-47b0-9edb-c6ef7166c0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"INSERT INTO BigData_DWH.branches_dimension SELECT * FROM branches_view\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11e3d6f-d33f-45cd-928a-aed2ae9190c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"INSERT INTO BigData_DWH.sales_agents_dimension SELECT * FROM agents_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bc9ebd-9991-405f-a252-5e173d24162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f9849d-f9c0-41ed-af1c-7a0b7362568c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
