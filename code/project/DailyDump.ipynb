{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0205d26c-eb0b-4757-804f-cf00bbee0b95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from directories matching pattern: /user/itversity/raw_layer/20240705/*\n",
      "Found 12 directories inside 20240705.\n",
      "Processing directory: hdfs://localhost:9000/user/itversity/raw_layer/20240705/11_group6\n",
      "Found 3 files in directory.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/11_group6/branches_SS_raw_6_11_20240705.csv\n",
      "Reading branches data.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/11_group6/sales_agents_SS_raw_6_11_20240705.csv\n",
      "Reading agents data.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/11_group6/sales_transactions_SS_raw_6_11_20240705.csv\n",
      "Reading transactions data.\n",
      "Processing directory: hdfs://localhost:9000/user/itversity/raw_layer/20240705/12_group1\n",
      "Found 4 files in directory.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/12_group1/_12_20240705.csv\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/12_group1/branches_SS_raw_1_12_20240705.csv\n",
      "Union with existing branches data.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/12_group1/sales_agents_SS_raw_1_12_20240705.csv\n",
      "Union with existing agents data.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/12_group1/sales_transactions_SS_raw_1_12_20240705.csv\n",
      "Union with existing transactions data.\n",
      "Processing directory: hdfs://localhost:9000/user/itversity/raw_layer/20240705/13_group2\n",
      "Found 3 files in directory.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/13_group2/branches_SS_raw_2_13_20240705.csv\n",
      "Union with existing branches data.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/13_group2/sales_agents_SS_raw_2_13_20240705.csv\n",
      "Union with existing agents data.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/13_group2/sales_transactions_SS_raw_2_13_20240705.csv\n",
      "Union with existing transactions data.\n",
      "Processing directory: hdfs://localhost:9000/user/itversity/raw_layer/20240705/14_group3\n",
      "Found 3 files in directory.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/14_group3/branches_SS_raw_3_14_20240705.csv\n",
      "Union with existing branches data.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/14_group3/sales_agents_SS_raw_3_14_20240705.csv\n",
      "Union with existing agents data.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/14_group3/sales_transactions_SS_raw_3_14_20240705.csv\n",
      "Union with existing transactions data.\n",
      "Processing directory: hdfs://localhost:9000/user/itversity/raw_layer/20240705/15_group4\n",
      "Found 3 files in directory.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/15_group4/branches_SS_raw_4_15_20240705.csv\n",
      "Union with existing branches data.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/15_group4/sales_agents_SS_raw_4_15_20240705.csv\n",
      "Union with existing agents data.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/15_group4/sales_transactions_SS_raw_4_15_20240705.csv\n",
      "Union with existing transactions data.\n",
      "Processing directory: hdfs://localhost:9000/user/itversity/raw_layer/20240705/16_group5\n",
      "Found 3 files in directory.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/16_group5/branches_SS_raw_5_16_20240705.csv\n",
      "Union with existing branches data.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/16_group5/sales_agents_SS_raw_5_16_20240705.csv\n",
      "Union with existing agents data.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/16_group5/sales_transactions_SS_raw_5_16_20240705.csv\n",
      "Union with existing transactions data.\n",
      "Processing directory: hdfs://localhost:9000/user/itversity/raw_layer/20240705/17_group6\n",
      "Found 3 files in directory.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/17_group6/branches_SS_raw_6_17_20240705.csv\n",
      "Union with existing branches data.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/17_group6/sales_agents_SS_raw_6_17_20240705.csv\n",
      "Union with existing agents data.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/17_group6/sales_transactions_SS_raw_6_17_20240705.csv\n",
      "Union with existing transactions data.\n",
      "Processing directory: hdfs://localhost:9000/user/itversity/raw_layer/20240705/18_group1\n",
      "Found 4 files in directory.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/18_group1/_18_20240705.csv\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/18_group1/branches_SS_raw_1_18_20240705.csv\n",
      "Union with existing branches data.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/18_group1/sales_agents_SS_raw_1_18_20240705.csv\n",
      "Union with existing agents data.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/18_group1/sales_transactions_SS_raw_1_18_20240705.csv\n",
      "Union with existing transactions data.\n",
      "Processing directory: hdfs://localhost:9000/user/itversity/raw_layer/20240705/19_group2\n",
      "Found 3 files in directory.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/19_group2/branches_SS_raw_2_19_20240705.csv\n",
      "Union with existing branches data.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/19_group2/sales_agents_SS_raw_2_19_20240705.csv\n",
      "Union with existing agents data.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/19_group2/sales_transactions_SS_raw_2_19_20240705.csv\n",
      "Union with existing transactions data.\n",
      "Processing directory: hdfs://localhost:9000/user/itversity/raw_layer/20240705/21_group3\n",
      "Found 3 files in directory.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/21_group3/branches_SS_raw_3_21_20240705.csv\n",
      "Union with existing branches data.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/21_group3/sales_agents_SS_raw_3_21_20240705.csv\n",
      "Union with existing agents data.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/21_group3/sales_transactions_SS_raw_3_21_20240705.csv\n",
      "Union with existing transactions data.\n",
      "Processing directory: hdfs://localhost:9000/user/itversity/raw_layer/20240705/22_group4\n",
      "Found 3 files in directory.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/22_group4/branches_SS_raw_4_22_20240705.csv\n",
      "Union with existing branches data.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/22_group4/sales_agents_SS_raw_4_22_20240705.csv\n",
      "Union with existing agents data.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/22_group4/sales_transactions_SS_raw_4_22_20240705.csv\n",
      "Union with existing transactions data.\n",
      "Processing directory: hdfs://localhost:9000/user/itversity/raw_layer/20240705/23_group5\n",
      "Found 3 files in directory.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/23_group5/branches_SS_raw_5_23_20240705.csv\n",
      "Union with existing branches data.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/23_group5/sales_agents_SS_raw_5_23_20240705.csv\n",
      "Union with existing agents data.\n",
      "Reading file: hdfs://localhost:9000/user/itversity/raw_layer/20240705/23_group5/sales_transactions_SS_raw_5_23_20240705.csv\n",
      "Union with existing transactions data.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Process and Insert Data into Daily Dump\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Calculate previous day date\n",
    "previous_day = (datetime.now() - timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "base_hdfs_dir = \"/user/itversity/raw_layer\"\n",
    "\n",
    "# Construct input path for the previous day's data\n",
    "input_dir_pattern = f\"{base_hdfs_dir}/{previous_day}/*\"\n",
    "\n",
    "branches_df = None\n",
    "agents_df = None\n",
    "transactions_df = None\n",
    "\n",
    "print(f\"Reading data from directories matching pattern: {input_dir_pattern}\")\n",
    "\n",
    "try:\n",
    "    # Get all directories inside the previous day's folder\n",
    "    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "    input_dirs = [status.getPath().toString() for status in fs.listStatus(spark._jvm.org.apache.hadoop.fs.Path(f\"{base_hdfs_dir}/{previous_day}\")) if status.isDirectory()]\n",
    "\n",
    "    print(f\"Found {len(input_dirs)} directories inside {previous_day}.\")\n",
    "\n",
    "    for input_dir in input_dirs:\n",
    "        print(f\"Processing directory: {input_dir}\")\n",
    "        \n",
    "        # Read all files in the directory\n",
    "        input_files = [status.getPath().toString() for status in fs.listStatus(spark._jvm.org.apache.hadoop.fs.Path(input_dir))]\n",
    "        \n",
    "        print(f\"Found {len(input_files)} files in directory.\")\n",
    "        \n",
    "        for input_file in input_files:\n",
    "            print(f\"Reading file: {input_file}\")\n",
    "            df = spark.read.format(\"csv\").option(\"header\", \"true\").load(input_file)\n",
    "            \n",
    "            # Drop 'source' and 'logs' columns if they exist\n",
    "            if 'source' in df.columns:\n",
    "                df = df.drop('source')\n",
    "            if 'logs' in df.columns:\n",
    "                df = df.drop('logs')\n",
    "            \n",
    "            # Ensure all DataFrames have the same schema before unioning\n",
    "            if \"branches\" in input_file.lower():\n",
    "                if branches_df is None:\n",
    "                    branches_df = df\n",
    "                    print(\"Reading branches data.\")\n",
    "                else:\n",
    "                    branches_df = branches_df.union(df)\n",
    "                    print(\"Union with existing branches data.\")\n",
    "            elif \"agents\" in input_file.lower():\n",
    "                if agents_df is None:\n",
    "                    agents_df = df\n",
    "                    print(\"Reading agents data.\")\n",
    "                else:\n",
    "                    agents_df = agents_df.union(df)\n",
    "                    print(\"Union with existing agents data.\")\n",
    "            elif \"transactions\" in input_file.lower():\n",
    "                if transactions_df is None:\n",
    "                    transactions_df = df\n",
    "                    print(\"Reading transactions data.\")\n",
    "                else:\n",
    "                    transactions_df = transactions_df.union(df)\n",
    "                    print(\"Union with existing transactions data.\")\n",
    "                    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acab8f28-52ce-43ec-ba18-a01f38d9de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "if branches_df:\n",
    "    branches_df = branches_df.dropDuplicates()\n",
    "if agents_df:\n",
    "    agents_df = agents_df.dropDuplicates()\n",
    "if transactions_df:\n",
    "    transactions_df = transactions_df.dropDuplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d697d97-e87f-4174-9ee0-e93df7101a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary view 'transactions_view' created successfully.\n",
      "Temporary view 'agents_view' created successfully.\n"
     ]
    }
   ],
   "source": [
    "if transactions_df:\n",
    "    transactions_df.createOrReplaceTempView(\"transactions_view\")\n",
    "    print(\"Temporary view 'transactions_view' created successfully.\")\n",
    "if agents_df:\n",
    "    agents_df.createOrReplaceTempView(\"agents_view\")\n",
    "    print(\"Temporary view 'agents_view' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4755f4d-f709-4326-be89-e38aab097022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+----------------+\n",
      "|  sales_agent_name|   product_name|total_sold_units|\n",
      "+------------------+---------------+----------------+\n",
      "|        Jane Smith|        Sandals|         24408.0|\n",
      "|        Jane Smith|Electric Kettle|         24210.0|\n",
      "|       Emma Taylor|     Hair Dryer|         26049.0|\n",
      "|   Michael Johnson|         Camera|         26109.0|\n",
      "|        Jane Smith|         Hoodie|         25962.0|\n",
      "|Christopher Miller|        Toaster|         24891.0|\n",
      "|        Jane Smith|          Jeans|         24819.0|\n",
      "|        Jane Smith|          Heels|         25275.0|\n",
      "|Christopher Miller|        Sandals|         25680.0|\n",
      "|          John Doe|        Printer|         25104.0|\n",
      "|   Michael Johnson|     Hair Dryer|         25689.0|\n",
      "|   Daniel Martinez|           Iron|         25887.0|\n",
      "|   Daniel Martinez|          Jeans|         25146.0|\n",
      "|      Olivia Davis|Electric Kettle|         25272.0|\n",
      "|      Olivia Davis|         Tablet|         24498.0|\n",
      "|      Sophia Moore|       Sneakers|         24564.0|\n",
      "|   Daniel Martinez|          Boots|         25659.0|\n",
      "|         john wick|      Microwave|         16126.0|\n",
      "|         john wick|          Heels|         17206.0|\n",
      "|      David Wilson|             TV|         24315.0|\n",
      "+------------------+---------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_query = \"\"\"\n",
    "    SELECT \n",
    "        a.name AS sales_agent_name, \n",
    "        t.product_name, \n",
    "        SUM(t.units) AS total_sold_units\n",
    "    FROM \n",
    "        transactions_view t\n",
    "    JOIN \n",
    "        agents_view a ON t.sales_agent_id = a.sales_person_id\n",
    "    GROUP BY \n",
    "        a.name, \n",
    "        t.product_name, \n",
    "        t.sales_agent_id, \n",
    "        t.product_id\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and store the result in a DataFrame\n",
    "result_df = spark.sql(sql_query)\n",
    "distinct_df = result_df.distinct()\n",
    "\n",
    "distinct_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6ff0f81-bac5-47f0-8976-a04a885820a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "previous_day = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "output_hdfs_path = f\"/user/itversity/daily_dump_from_source/{previous_day}\"\n",
    "\n",
    "distinct_df.coalesce(1).write.mode(\"overwrite\").csv(output_hdfs_path, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e9211be-8789-4fb5-ad47-38dc93fb48f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/user/itversity/daily_dump_from_source/2024-07-05' copied successfully to 'daily_dump_from_source'.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "output_hdfs_path = \"/user/itversity/daily_dump_from_source/2024-07-05\"\n",
    "local_output_path = \"daily_dump_from_source\"\n",
    "\n",
    "try:\n",
    "    # Copy the directory from HDFS to the local file system\n",
    "    copy_command = ['hadoop', 'fs', '-get', '-f', output_hdfs_path, local_output_path]\n",
    "    copy_result = subprocess.run(copy_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)\n",
    "\n",
    "    if copy_result.returncode == 0:\n",
    "        print(f\"Directory '{output_hdfs_path}' copied successfully to '{local_output_path}'.\")\n",
    "    else:\n",
    "        print(f\"Error: {copy_result.stderr.decode('utf-8')}\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error: {e.stderr.decode('utf-8')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d55f2f4c-4cd2-4126-ad2b-c97cdb0c6b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510749e2-88a3-4c5a-b5b4-b9ffa0f408f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
